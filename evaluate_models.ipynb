{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models using map_records_under_heading\n",
    "\n",
    "`map_records_under_heading.csv` contains the links between taxonomy and records.\n",
    "\n",
    "Each taxonomy corresponds to one or more records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, XLNetTokenizer, XLNetModel, AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from pre_processing import PreProcessing\n",
    "import random\n",
    "\n",
    "device = torch.device(\"mps\" if getattr(torch,'has_mps',False) else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>child_id</th>\n",
       "      <th>item_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_at           updated_at  parent_id  child_id  \\\n",
       "0   1  2020-12-28 01:28:05  2020-12-28 01:28:05          3         1   \n",
       "1   2  2020-12-28 01:28:05  2020-12-28 01:28:05          8         1   \n",
       "2   3  2020-12-28 01:28:05  2020-12-28 01:28:05         12         1   \n",
       "3   4  2020-12-28 01:28:05  2020-12-28 01:28:05         13         1   \n",
       "4   5  2020-12-28 01:28:05  2020-12-28 01:28:05         14         1   \n",
       "5   6  2020-12-28 01:28:05  2020-12-28 01:28:05         16         1   \n",
       "6   7  2020-12-28 01:28:05  2020-12-28 01:28:05         17         1   \n",
       "7   8  2020-12-28 01:28:05  2020-12-28 01:28:05         19         1   \n",
       "8   9  2020-12-28 01:28:05  2020-12-28 01:28:05         20         1   \n",
       "9  10  2020-12-28 01:28:05  2020-12-28 01:28:05         21         1   \n",
       "\n",
       "   item_order  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_records_heading = pd.read_csv('data/map_records_under_heading.csv')\n",
    "map_records_heading.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_records = list(zip(map_records_heading['parent_id'], map_records_heading['child_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>translations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Root</td>\n",
       "      <td>Root</td>\n",
       "      <td>{\"name\":{\"en\":\"Root\",\"fr\":null},\"description\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All Mental Health Resources</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tThe listings of mental health resourc...</td>\n",
       "      <td>{\"name\":{\"en\":\"All Mental Health Resources\",\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Crisis and Emergency</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tRefers to all programs that provide i...</td>\n",
       "      <td>{\"name\":{\"en\":\"Crisis and Emergency\",\"fr\":\"Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>System Navigation, including Information and R...</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAre you looking for help, but don&amp;#39...</td>\n",
       "      <td>{\"name\":{\"en\":\"System Navigation, including In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Child Welfare including Children's Aid Society...</td>\n",
       "      <td>&lt;p&gt;The child welfare / child protection system...</td>\n",
       "      <td>{\"name\":{\"en\":\"Child Welfare including Childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Emergency Shelter and Housing</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tThere are various shelters that peopl...</td>\n",
       "      <td>{\"name\":{\"en\":\"Emergency Shelter and Housing\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Hospital Emergency Department</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tIs there an emergency such as medical...</td>\n",
       "      <td>{\"name\":{\"en\":\"Hospital Emergency Department\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Crisis Lines including Telephone, Online and Chat</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAre you in a crisis? Crisis lines off...</td>\n",
       "      <td>{\"name\":{\"en\":\"Crisis Lines including Telephon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Psychiatrists</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tPsychiatrists are medical doctors who...</td>\n",
       "      <td>{\"name\":{\"en\":\"Psychiatrists\",\"fr\":\"Psychiatre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>A-Z Mental Health Conditions and Topics</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAlphabetical list of mental health to...</td>\n",
       "      <td>{\"name\":{\"en\":\"A-Z Mental Health Conditions an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               name  \\\n",
       "0   1                                               Root   \n",
       "1   2                        All Mental Health Resources   \n",
       "2   3                               Crisis and Emergency   \n",
       "3   4  System Navigation, including Information and R...   \n",
       "4   5  Child Welfare including Children's Aid Society...   \n",
       "5   6                      Emergency Shelter and Housing   \n",
       "6   7                      Hospital Emergency Department   \n",
       "7   8  Crisis Lines including Telephone, Online and Chat   \n",
       "8   9                                      Psychiatrists   \n",
       "9  10            A-Z Mental Health Conditions and Topics   \n",
       "\n",
       "                                         description  \\\n",
       "0                                               Root   \n",
       "1  <p>\\r\\n\\tThe listings of mental health resourc...   \n",
       "2  <p>\\r\\n\\tRefers to all programs that provide i...   \n",
       "3  <p>\\r\\n\\tAre you looking for help, but don&#39...   \n",
       "4  <p>The child welfare / child protection system...   \n",
       "5  <p>\\r\\n\\tThere are various shelters that peopl...   \n",
       "6  <p>\\r\\n\\tIs there an emergency such as medical...   \n",
       "7  <p>\\r\\n\\tAre you in a crisis? Crisis lines off...   \n",
       "8  <p>\\r\\n\\tPsychiatrists are medical doctors who...   \n",
       "9  <p>\\r\\n\\tAlphabetical list of mental health to...   \n",
       "\n",
       "                                        translations  \n",
       "0  {\"name\":{\"en\":\"Root\",\"fr\":null},\"description\":...  \n",
       "1  {\"name\":{\"en\":\"All Mental Health Resources\",\"f...  \n",
       "2  {\"name\":{\"en\":\"Crisis and Emergency\",\"fr\":\"Res...  \n",
       "3  {\"name\":{\"en\":\"System Navigation, including In...  \n",
       "4  {\"name\":{\"en\":\"Child Welfare including Childre...  \n",
       "5  {\"name\":{\"en\":\"Emergency Shelter and Housing\",...  \n",
       "6  {\"name\":{\"en\":\"Hospital Emergency Department\",...  \n",
       "7  {\"name\":{\"en\":\"Crisis Lines including Telephon...  \n",
       "8  {\"name\":{\"en\":\"Psychiatrists\",\"fr\":\"Psychiatre...  \n",
       "9  {\"name\":{\"en\":\"A-Z Mental Health Conditions an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = pd.read_json('data/records.json')\n",
    "records = records.drop(['created_at', 'updated_at', 'deleted_at', 'publish', 'academic_credentials', 'age_max', 'age_min', 'last_name', \n",
    "                        'latitude', 'longitude', 'name_of_private_practice', 'fee_description',\t'fee_type',\t'first_name',\t'languages',\n",
    "                        'organization_type', 'original_id',\t'record_type',\t'salutation_type', 'website'], axis=1)\n",
    "\n",
    "taxonomy = pd.read_json('data/taxonomy_headings.json')\n",
    "taxonomy = taxonomy.drop(['created_at',\t'updated_at',\t'deleted_at', 'alias_of_id', 'short_description',\t'original_id'], axis=1)\n",
    "\n",
    "taxonomy.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of records before preprocessing: 6406\n",
      "Length of taxonomy before preprocessing: 277\n",
      "Length of records after preprocessing: 6239\n",
      "Length of taxonomy after preprocessing: 192\n"
     ]
    }
   ],
   "source": [
    "records, taxonomy = PreProcessing(records, taxonomy).preprocess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly chose ten taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53: Psychotherapists\n",
      "Psychotherapists provide psychotherapy (aka talk therapy) and help individuals with difficulties by listening and giving support.Special training is required in order to become a psychotherapist.Various types of professionals such asSocial Workers or Psychologists provide psychotherapy, and can thus be viewed as being psychotherapists as well.\n",
      "\n",
      "120: Cancer\n",
      "Living with cancer can mean living with a wide range of emotions and psychological stresses that can have an impact on mental health and well-being.\n",
      "\n",
      "58: Counselling and Therapy\n",
      "Counselling and therapy can be helpful for dealing with a wide variety of issues (e.g. stress and coping, relationship problems)and mental health conditions (e.g. depression, anxiety, etc.)\n",
      "\n",
      "89: Grief and Bereavement\n",
      "Grief and bereavement refers to the sadness and loneliness that result from the loss of a loved one.\n",
      "\n",
      "175: Acceptance and Commitment Therapy (ACT)\n",
      "ACT is a type of therapy that helps people by using acceptance and mindfulness, along with commitment and behaviour-change strategies.\n",
      "\n",
      "168: Psychoeducational Assessment\n",
      "When an individual has learning problems, it can be very helpful to have a psychoeducational assessment performed by a psychologist.Ways to find a psychologist include:Talk with the childs school regarding the possibility of seeing a psychologist that works with the schoolboard. This is paid for by the schoolboard, however there are often long waitlists.Agencies (such as psychology training programs with a university) that can offer free, or low-cost testing.Seeing a psychologist working in private practice, which is usually the quickest option. In many cases, a family members employee health plan may pay some of this expense. If you are having problems finding psychologists in this category, consider just looking directly under psychologists.\n",
      "\n",
      "61: Eye Movement Desensitization and Reprocessing (EMDR)\n",
      "Eye Movement Desensitization and Reprocessing (EMDR) is a treatment reported as being helpful in various conditions such as anxiety, and trauma (such as post-traumatic stress disorder).\n",
      "\n",
      "17: Custody and Access (of Children)\n",
      "Custody and access refers to the process of making informed determinations about custody and access in order to serve the best interests of the child.\n",
      "\n",
      "95: Music Therapy\n",
      "Music therapy is the use of music by an accredited music therapist to promote, maintain, and restore mental, physical, emotional, and spiritual health.\n",
      "\n",
      "129: Justice Sector (including Law Enforcement, Justice and Corrections)\n",
      "Individuals with mental health needs may come into contact with the Justice Sector, and similarly, those within the Justice Sector may encounter those with mental health needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate ten random integers between 0 and 191\n",
    "random_ids = [random.randint(0, 191) for i in range(10)]\n",
    "search_terms = []\n",
    "for each in random_ids:\n",
    "    print(str(each) + ': ' + taxonomy.iloc[each]['name'] + '\\n' + taxonomy.iloc[each]['description'] + '\\n')\n",
    "    search_terms.append(taxonomy.iloc[each]['name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(record_file_path, taxonomy_file_path):\n",
    "    # Load embeddings from pt\n",
    "    record_embeddings = torch.load(record_file_path)\n",
    "    taxonomy_embeddings = torch.load(taxonomy_file_path)\n",
    "    return record_embeddings, taxonomy_embeddings\n",
    "\n",
    "def get_highest_numbers_with_indices(numbers, n=10):\n",
    "    \"\"\"\n",
    "    Returns the n highest numbers in a list along with their indices.\n",
    "    :param numbers: List of numbers\n",
    "    :param n: Number of highest numbers to retrieve (default: 10)\n",
    "    :return: List of tuples containing the highest numbers and their indices\n",
    "    \"\"\"\n",
    "    highest_numbers_with_indices = []\n",
    "    for i, num in enumerate(numbers):\n",
    "        if len(highest_numbers_with_indices) < n:\n",
    "            highest_numbers_with_indices.append((num, i))\n",
    "            highest_numbers_with_indices.sort(reverse=True)\n",
    "        else:\n",
    "            if num > highest_numbers_with_indices[-1][0]:\n",
    "                highest_numbers_with_indices.pop()\n",
    "                highest_numbers_with_indices.append((num, i))\n",
    "                highest_numbers_with_indices.sort(reverse=True)\n",
    "    return highest_numbers_with_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at AIMH/mental-bert-large-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"AIMH/mental-bert-large-uncased\")\n",
    "model = BertModel.from_pretrained(\"AIMH/mental-bert-large-uncased\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'],\n",
    "                    token_type_ids=tokenized_search_terms['token_type_ids'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 1024])\n",
      "torch.Size([192, 1024])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/bert_records_embeddings.pt', 'data/embeddings/bert_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 5693)</td>\n",
       "      <td>0.807460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 1634)</td>\n",
       "      <td>0.807221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 3809)</td>\n",
       "      <td>0.777932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 3940)</td>\n",
       "      <td>0.777818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 620)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 618)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 617)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 616)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 614)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 613)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 5693)          0.807460\n",
       "1      (53, 1634)          0.807221\n",
       "2      (53, 3809)          0.777932\n",
       "3      (53, 3940)          0.777818\n",
       "4       (53, 620)          0.773732\n",
       "5       (53, 618)          0.773732\n",
       "6       (53, 617)          0.773732\n",
       "7       (53, 616)          0.773732\n",
       "8       (53, 614)          0.773732\n",
       "9       (53, 613)          0.773732"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.01\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at AIMH/mental-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('AIMH/mental-roberta-large')\n",
    "model = RobertaModel.from_pretrained('AIMH/mental-roberta-large').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 1024])\n",
      "torch.Size([192, 1024])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/roberta_records_embeddings.pt', 'data/embeddings/roberta_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 1634)</td>\n",
       "      <td>0.877737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 5147)</td>\n",
       "      <td>0.877365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 5463)</td>\n",
       "      <td>0.865646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 2683)</td>\n",
       "      <td>0.853112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 5693)</td>\n",
       "      <td>0.844657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 3940)</td>\n",
       "      <td>0.837582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 1806)</td>\n",
       "      <td>0.826826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 1761)</td>\n",
       "      <td>0.814847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 1525)</td>\n",
       "      <td>0.802147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 2455)</td>\n",
       "      <td>0.800418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 1634)          0.877737\n",
       "1      (53, 5147)          0.877365\n",
       "2      (53, 5463)          0.865646\n",
       "3      (53, 2683)          0.853112\n",
       "4      (53, 5693)          0.844657\n",
       "5      (53, 3940)          0.837582\n",
       "6      (53, 1806)          0.826826\n",
       "7      (53, 1761)          0.814847\n",
       "8      (53, 1525)          0.802147\n",
       "9      (53, 2455)          0.800418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.05\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetModel(\n",
       "  (word_embedding): Embedding(32000, 768)\n",
       "  (layer): ModuleList(\n",
       "    (0-11): 12 x XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_function): GELUActivation()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('AIMH/mental-xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('AIMH/mental-xlnet-base-cased').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'],\n",
    "                    token_type_ids=tokenized_search_terms['token_type_ids'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 768])\n",
      "torch.Size([192, 768])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/xlnet_records_embeddings.pt', 'data/embeddings/xlnet_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 5794)</td>\n",
       "      <td>0.820551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 1749)</td>\n",
       "      <td>0.804476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 4354)</td>\n",
       "      <td>0.764771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 3945)</td>\n",
       "      <td>0.761853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 5016)</td>\n",
       "      <td>0.748078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 5946)</td>\n",
       "      <td>0.740098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 3144)</td>\n",
       "      <td>0.735447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 5463)</td>\n",
       "      <td>0.734411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 3130)</td>\n",
       "      <td>0.732243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 3362)</td>\n",
       "      <td>0.731013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 5794)          0.820551\n",
       "1      (53, 1749)          0.804476\n",
       "2      (53, 4354)          0.764771\n",
       "3      (53, 3945)          0.761853\n",
       "4      (53, 5016)          0.748078\n",
       "5      (53, 5946)          0.740098\n",
       "6      (53, 3144)          0.735447\n",
       "7      (53, 5463)          0.734411\n",
       "8      (53, 3130)          0.732243\n",
       "9      (53, 3362)          0.731013"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.02\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Get SGPT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-1.3B-weightedmean-nli-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-1.3B-weightedmean-nli-bitfit\").to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(input_ids, attention_mask):\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "        last_hidden_state = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=600, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = getEmbeddings(tokenized_search_terms['input_ids'], attention_mask=tokenized_search_terms['attention_mask']).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 768])\n",
      "torch.Size([192, 768])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/sgpt_records_embeddings.pt', 'data/embeddings/sgpt_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(184, 1634)</td>\n",
       "      <td>0.780829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(184, 5000)</td>\n",
       "      <td>0.702536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(184, 6055)</td>\n",
       "      <td>0.656871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(184, 2586)</td>\n",
       "      <td>0.646647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(184, 4978)</td>\n",
       "      <td>0.613448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(184, 1761)</td>\n",
       "      <td>0.611580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(184, 3809)</td>\n",
       "      <td>0.606449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(184, 5016)</td>\n",
       "      <td>0.600357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(184, 5880)</td>\n",
       "      <td>0.595736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(184, 5279)</td>\n",
       "      <td>0.595647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0     (184, 1634)          0.780829\n",
       "1     (184, 5000)          0.702536\n",
       "2     (184, 6055)          0.656871\n",
       "3     (184, 2586)          0.646647\n",
       "4     (184, 4978)          0.613448\n",
       "5     (184, 1761)          0.611580\n",
       "6     (184, 3809)          0.606449\n",
       "7     (184, 5016)          0.600357\n",
       "8     (184, 5880)          0.595736\n",
       "9     (184, 5279)          0.595647"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6900",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
