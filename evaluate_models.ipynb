{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models using map_records_under_heading\n",
    "\n",
    "`map_records_under_heading.csv` contains the links between taxonomy and records.\n",
    "\n",
    "Each taxonomy corresponds to one or more records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, XLNetTokenizer, XLNetModel, AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from pre_processing import PreProcessing\n",
    "import random\n",
    "\n",
    "device = torch.device(\"mps\" if getattr(torch,'has_mps',False) else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>child_id</th>\n",
       "      <th>item_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>2020-12-28 01:28:05</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_at           updated_at  parent_id  child_id  \\\n",
       "0   1  2020-12-28 01:28:05  2020-12-28 01:28:05          3         1   \n",
       "1   2  2020-12-28 01:28:05  2020-12-28 01:28:05          8         1   \n",
       "2   3  2020-12-28 01:28:05  2020-12-28 01:28:05         12         1   \n",
       "3   4  2020-12-28 01:28:05  2020-12-28 01:28:05         13         1   \n",
       "4   5  2020-12-28 01:28:05  2020-12-28 01:28:05         14         1   \n",
       "5   6  2020-12-28 01:28:05  2020-12-28 01:28:05         16         1   \n",
       "6   7  2020-12-28 01:28:05  2020-12-28 01:28:05         17         1   \n",
       "7   8  2020-12-28 01:28:05  2020-12-28 01:28:05         19         1   \n",
       "8   9  2020-12-28 01:28:05  2020-12-28 01:28:05         20         1   \n",
       "9  10  2020-12-28 01:28:05  2020-12-28 01:28:05         21         1   \n",
       "\n",
       "   item_order  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_records_heading = pd.read_csv('data/map_records_under_heading.csv')\n",
    "map_records_heading.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_records = list(zip(map_records_heading['parent_id'], map_records_heading['child_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>translations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Root</td>\n",
       "      <td>Root</td>\n",
       "      <td>{\"name\":{\"en\":\"Root\",\"fr\":null},\"description\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All Mental Health Resources</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tThe listings of mental health resourc...</td>\n",
       "      <td>{\"name\":{\"en\":\"All Mental Health Resources\",\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Crisis and Emergency</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tRefers to all programs that provide i...</td>\n",
       "      <td>{\"name\":{\"en\":\"Crisis and Emergency\",\"fr\":\"Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>System Navigation, including Information and R...</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAre you looking for help, but don&amp;#39...</td>\n",
       "      <td>{\"name\":{\"en\":\"System Navigation, including In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Child Welfare including Children's Aid Society...</td>\n",
       "      <td>&lt;p&gt;The child welfare / child protection system...</td>\n",
       "      <td>{\"name\":{\"en\":\"Child Welfare including Childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Emergency Shelter and Housing</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tThere are various shelters that peopl...</td>\n",
       "      <td>{\"name\":{\"en\":\"Emergency Shelter and Housing\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Hospital Emergency Department</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tIs there an emergency such as medical...</td>\n",
       "      <td>{\"name\":{\"en\":\"Hospital Emergency Department\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Crisis Lines including Telephone, Online and Chat</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAre you in a crisis? Crisis lines off...</td>\n",
       "      <td>{\"name\":{\"en\":\"Crisis Lines including Telephon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Psychiatrists</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tPsychiatrists are medical doctors who...</td>\n",
       "      <td>{\"name\":{\"en\":\"Psychiatrists\",\"fr\":\"Psychiatre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>A-Z Mental Health Conditions and Topics</td>\n",
       "      <td>&lt;p&gt;\\r\\n\\tAlphabetical list of mental health to...</td>\n",
       "      <td>{\"name\":{\"en\":\"A-Z Mental Health Conditions an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               name  \\\n",
       "0   1                                               Root   \n",
       "1   2                        All Mental Health Resources   \n",
       "2   3                               Crisis and Emergency   \n",
       "3   4  System Navigation, including Information and R...   \n",
       "4   5  Child Welfare including Children's Aid Society...   \n",
       "5   6                      Emergency Shelter and Housing   \n",
       "6   7                      Hospital Emergency Department   \n",
       "7   8  Crisis Lines including Telephone, Online and Chat   \n",
       "8   9                                      Psychiatrists   \n",
       "9  10            A-Z Mental Health Conditions and Topics   \n",
       "\n",
       "                                         description  \\\n",
       "0                                               Root   \n",
       "1  <p>\\r\\n\\tThe listings of mental health resourc...   \n",
       "2  <p>\\r\\n\\tRefers to all programs that provide i...   \n",
       "3  <p>\\r\\n\\tAre you looking for help, but don&#39...   \n",
       "4  <p>The child welfare / child protection system...   \n",
       "5  <p>\\r\\n\\tThere are various shelters that peopl...   \n",
       "6  <p>\\r\\n\\tIs there an emergency such as medical...   \n",
       "7  <p>\\r\\n\\tAre you in a crisis? Crisis lines off...   \n",
       "8  <p>\\r\\n\\tPsychiatrists are medical doctors who...   \n",
       "9  <p>\\r\\n\\tAlphabetical list of mental health to...   \n",
       "\n",
       "                                        translations  \n",
       "0  {\"name\":{\"en\":\"Root\",\"fr\":null},\"description\":...  \n",
       "1  {\"name\":{\"en\":\"All Mental Health Resources\",\"f...  \n",
       "2  {\"name\":{\"en\":\"Crisis and Emergency\",\"fr\":\"Res...  \n",
       "3  {\"name\":{\"en\":\"System Navigation, including In...  \n",
       "4  {\"name\":{\"en\":\"Child Welfare including Childre...  \n",
       "5  {\"name\":{\"en\":\"Emergency Shelter and Housing\",...  \n",
       "6  {\"name\":{\"en\":\"Hospital Emergency Department\",...  \n",
       "7  {\"name\":{\"en\":\"Crisis Lines including Telephon...  \n",
       "8  {\"name\":{\"en\":\"Psychiatrists\",\"fr\":\"Psychiatre...  \n",
       "9  {\"name\":{\"en\":\"A-Z Mental Health Conditions an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = pd.read_json('data/records.json')\n",
    "records = records.drop(['created_at', 'updated_at', 'deleted_at', 'publish', 'academic_credentials', 'age_max', 'age_min', 'last_name', \n",
    "                        'latitude', 'longitude', 'name_of_private_practice', 'fee_description',\t'fee_type',\t'first_name',\t'languages',\n",
    "                        'organization_type', 'original_id',\t'record_type',\t'salutation_type', 'website'], axis=1)\n",
    "\n",
    "taxonomy = pd.read_json('data/taxonomy_headings.json')\n",
    "taxonomy = taxonomy.drop(['created_at',\t'updated_at',\t'deleted_at', 'alias_of_id', 'short_description',\t'original_id'], axis=1)\n",
    "\n",
    "taxonomy.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of records before preprocessing: 6406\n",
      "Length of taxonomy before preprocessing: 277\n",
      "Length of records after preprocessing: 6239\n",
      "Length of taxonomy after preprocessing: 192\n"
     ]
    }
   ],
   "source": [
    "records, taxonomy = PreProcessing(records, taxonomy).preprocess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly chose ten taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178: E-Counseling, E-Therapy, Virtual Therapy, Distance Therapy\n",
      "Online therapy (aka e-counseling, distance therapy) is therapy delivered by a real person over the internet.Traditionally therapy has been face-to-face, but especially since the pandemic of 2020, most providers offer remote and virtual services such as video chat (through Facetime, Skype, Zoom, MS Teams, etc.).\n",
      "\n",
      "151: Independent Schools\n",
      "Independent schools (also known as private schools) are schools that are funded through tuition fees and other means, as opposed to being funded publicly by tax dollars. Many independent schools have classroom environments that can provide specific support to students with learning and mental health needs.\n",
      "\n",
      "179: Visual Stress\n",
      "Visual stress is a visual perceptual processing condition that affects how visual information is interpreted by the brain and interferes with reading, attention, coordination, general health and behaviour. This is different from problems involving sight or sharpness of vision and can occur despite normal vision. Classic symptoms include light sensitivity, headaches from reading, and problems reading because the white page appears too bright or the words appear to be moving, flashing, or jumping on the page. As reading is such a key skill for school and life in general, problems with reading can thus lead to significant impairment. The good news is that appropriate intervention can make a significant improvement and for many individuals, one of the interventions is as simple as specific colour filters.\n",
      "\n",
      "123: Service Area Types\n",
      "Service Types Test-English\n",
      "\n",
      "147: In Conflict with the Law\n",
      "People in conflict with the law may have mental health needs, and unmet mental health needs may also lead to problems with the law.\n",
      "\n",
      "43: Medical Services\n",
      "A healthy mind and spirit relies on having a healthy body as well, and this section has information on key medical resources of interest to those with mental health issues. This is not an exhaustive list of resources and services, but highlights those which might be of interest to individuals with mental health issues.\n",
      "\n",
      "4: Child Welfare including Children's Aid Society (CAS)\n",
      "The child welfare / child protection system is a group of services that promote the well-being of children by protecting children from abuse or neglect, ensuring they are safe, and strengthening families so that they can successfully care for their children.\n",
      "\n",
      "89: Grief and Bereavement\n",
      "Grief and bereavement refers to the sadness and loneliness that result from the loss of a loved one.\n",
      "\n",
      "171: Oppositional behaviours including oppositional defiant disorder (ODD)\n",
      "Oppositional behaviours are when a child/youth has troubles following expectations from others (i.e. being disobedient) and can be hostile at adults. While some oppositionality and independence is normal for all children, when it is severe and causes problems, it may be a sign of oppositional defiant disorder (ODD).\n",
      "\n",
      "65: Art Therapy\n",
      "Art therapy is a way of helping people through art, and is provided by accredited art therapists. Painting, drawing and sculpting are common ways that help people to express themselves and heal. Unlike traditional art, the focus is on the process of creating the artwork rather than the final product.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate ten random integers between 0 and 191\n",
    "random_ids = [random.randint(0, 191) for i in range(10)]\n",
    "search_terms = []\n",
    "for each in random_ids:\n",
    "    print(str(each) + ': ' + taxonomy.iloc[each]['name'] + '\\n' + taxonomy.iloc[each]['description'] + '\\n')\n",
    "    search_terms.append(taxonomy.iloc[each]['name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(record_file_path, taxonomy_file_path):\n",
    "    # Load embeddings from pt\n",
    "    record_embeddings = torch.load(record_file_path)\n",
    "    taxonomy_embeddings = torch.load(taxonomy_file_path)\n",
    "    return record_embeddings, taxonomy_embeddings\n",
    "\n",
    "def get_highest_numbers_with_indices(numbers, n=10):\n",
    "    \"\"\"\n",
    "    Returns the n highest numbers in a list along with their indices.\n",
    "    :param numbers: List of numbers\n",
    "    :param n: Number of highest numbers to retrieve (default: 10)\n",
    "    :return: List of tuples containing the highest numbers and their indices\n",
    "    \"\"\"\n",
    "    highest_numbers_with_indices = []\n",
    "    for i, num in enumerate(numbers):\n",
    "        if len(highest_numbers_with_indices) < n:\n",
    "            highest_numbers_with_indices.append((num, i))\n",
    "            highest_numbers_with_indices.sort(reverse=True)\n",
    "        else:\n",
    "            if num > highest_numbers_with_indices[-1][0]:\n",
    "                highest_numbers_with_indices.pop()\n",
    "                highest_numbers_with_indices.append((num, i))\n",
    "                highest_numbers_with_indices.sort(reverse=True)\n",
    "    return highest_numbers_with_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at AIMH/mental-bert-large-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"AIMH/mental-bert-large-uncased\")\n",
    "model = BertModel.from_pretrained(\"AIMH/mental-bert-large-uncased\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'],\n",
    "                    token_type_ids=tokenized_search_terms['token_type_ids'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 1024])\n",
      "torch.Size([192, 1024])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/bert_records_embeddings.pt', 'data/embeddings/bert_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 5693)</td>\n",
       "      <td>0.807460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 1634)</td>\n",
       "      <td>0.807221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 3809)</td>\n",
       "      <td>0.777932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 3940)</td>\n",
       "      <td>0.777818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 620)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 618)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 617)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 616)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 614)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 613)</td>\n",
       "      <td>0.773732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 5693)          0.807460\n",
       "1      (53, 1634)          0.807221\n",
       "2      (53, 3809)          0.777932\n",
       "3      (53, 3940)          0.777818\n",
       "4       (53, 620)          0.773732\n",
       "5       (53, 618)          0.773732\n",
       "6       (53, 617)          0.773732\n",
       "7       (53, 616)          0.773732\n",
       "8       (53, 614)          0.773732\n",
       "9       (53, 613)          0.773732"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.01\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at AIMH/mental-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('AIMH/mental-roberta-large')\n",
    "model = RobertaModel.from_pretrained('AIMH/mental-roberta-large').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 1024])\n",
      "torch.Size([192, 1024])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/roberta_records_embeddings.pt', 'data/embeddings/roberta_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 1634)</td>\n",
       "      <td>0.877737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 5147)</td>\n",
       "      <td>0.877365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 5463)</td>\n",
       "      <td>0.865646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 2683)</td>\n",
       "      <td>0.853112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 5693)</td>\n",
       "      <td>0.844657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 3940)</td>\n",
       "      <td>0.837582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 1806)</td>\n",
       "      <td>0.826826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 1761)</td>\n",
       "      <td>0.814847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 1525)</td>\n",
       "      <td>0.802147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 2455)</td>\n",
       "      <td>0.800418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 1634)          0.877737\n",
       "1      (53, 5147)          0.877365\n",
       "2      (53, 5463)          0.865646\n",
       "3      (53, 2683)          0.853112\n",
       "4      (53, 5693)          0.844657\n",
       "5      (53, 3940)          0.837582\n",
       "6      (53, 1806)          0.826826\n",
       "7      (53, 1761)          0.814847\n",
       "8      (53, 1525)          0.802147\n",
       "9      (53, 2455)          0.800418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.05\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AIMH/mental-xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetModel(\n",
       "  (word_embedding): Embedding(32000, 768)\n",
       "  (layer): ModuleList(\n",
       "    (0-11): 12 x XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation_function): GELUActivation()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('AIMH/mental-xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('AIMH/mental-xlnet-base-cased').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=512, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = []\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=tokenized_search_terms['input_ids'],\n",
    "                    attention_mask=tokenized_search_terms['attention_mask'],\n",
    "                    token_type_ids=tokenized_search_terms['token_type_ids'])\n",
    "    embedding = embedding.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "search_term_embeddings = embedding.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 768])\n",
      "torch.Size([192, 768])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/xlnet_records_embeddings.pt', 'data/embeddings/xlnet_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonomy_record</th>\n",
       "      <th>Similarity score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(53, 5794)</td>\n",
       "      <td>0.820551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(53, 1749)</td>\n",
       "      <td>0.804476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(53, 4354)</td>\n",
       "      <td>0.764771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 3945)</td>\n",
       "      <td>0.761853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(53, 5016)</td>\n",
       "      <td>0.748078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(53, 5946)</td>\n",
       "      <td>0.740098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(53, 3144)</td>\n",
       "      <td>0.735447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(53, 5463)</td>\n",
       "      <td>0.734411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53, 3130)</td>\n",
       "      <td>0.732243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53, 3362)</td>\n",
       "      <td>0.731013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taxonomy_record  Similarity score\n",
       "0      (53, 5794)          0.820551\n",
       "1      (53, 1749)          0.804476\n",
       "2      (53, 4354)          0.764771\n",
       "3      (53, 3945)          0.761853\n",
       "4      (53, 5016)          0.748078\n",
       "5      (53, 5946)          0.740098\n",
       "6      (53, 3144)          0.735447\n",
       "7      (53, 5463)          0.734411\n",
       "8      (53, 3130)          0.732243\n",
       "9      (53, 3362)          0.731013"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.02\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoModel(\n",
       "  (wte): Embedding(50257, 2048)\n",
       "  (wpe): Embedding(2048, 2048)\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-23): 24 x GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get SGPT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-1.3B-weightedmean-nli-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-1.3B-weightedmean-nli-bitfit\").to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(input_ids, attention_mask):\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "        last_hidden_state = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_search_terms = tokenizer(search_terms, padding='max_length', max_length=600, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "search_term_embeddings = getEmbeddings(tokenized_search_terms['input_ids'], attention_mask=tokenized_search_terms['attention_mask']).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6239, 768])\n",
      "torch.Size([192, 768])\n"
     ]
    }
   ],
   "source": [
    "record_embeddings, taxonomy_embeddings = load_embeddings('data/embeddings/sgpt_records_embeddings.pt', 'data/embeddings/sgpt_taxonomy_embeddings.pt')\n",
    "print(record_embeddings.shape)\n",
    "print(taxonomy_embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2048,) (768,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m cos_sim \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m record_embeddings:\n\u001b[0;32m----> 7\u001b[0m     cos_sim\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m cosine(search_term_embeddings[idx], each))\n\u001b[1;32m      9\u001b[0m lst \u001b[39m=\u001b[39m get_highest_numbers_with_indices(cos_sim)\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m lst:\n",
      "File \u001b[0;32m~/miniconda3/envs/6900/lib/python3.10/site-packages/scipy/spatial/distance.py:670\u001b[0m, in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m \n\u001b[1;32m    666\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#   or 'reflective correlation'\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39m# clamp the result to 0-2\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mmin\u001b[39m(correlation(u, v, w\u001b[39m=\u001b[39;49mw, centered\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), \u001b[39m2.0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/6900/lib/python3.10/site-packages/scipy/spatial/distance.py:619\u001b[0m, in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    617\u001b[0m     u \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m umu\n\u001b[1;32m    618\u001b[0m     v \u001b[39m=\u001b[39m v \u001b[39m-\u001b[39m vmu\n\u001b[0;32m--> 619\u001b[0m uv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(u \u001b[39m*\u001b[39;49m v, weights\u001b[39m=\u001b[39mw)\n\u001b[1;32m    620\u001b[0m uu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39msquare(u), weights\u001b[39m=\u001b[39mw)\n\u001b[1;32m    621\u001b[0m vv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39msquare(v), weights\u001b[39m=\u001b[39mw)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2048,) (768,) "
     ]
    }
   ],
   "source": [
    "records_result = {'taxonomy_record': [], 'Similarity score': []}\n",
    "\n",
    "for idx in range(len(search_term_embeddings)):\n",
    "    # Records\n",
    "    cos_sim = []\n",
    "    for each in record_embeddings:\n",
    "        cos_sim.append(1 - cosine(search_term_embeddings[idx], each))\n",
    "\n",
    "    lst = get_highest_numbers_with_indices(cos_sim)\n",
    "\n",
    "    for id in lst:\n",
    "        records_result['taxonomy_record'].append((random_ids[idx], id[1]))\n",
    "        records_result['Similarity score'].append(id[0])\n",
    "\n",
    "records_result = pd.DataFrame(records_result)\n",
    "records_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06\n"
     ]
    }
   ],
   "source": [
    "retrieved_relevant = 0\n",
    "for each in records_result['taxonomy_record'].to_list():\n",
    "    if each in taxonomy_records:\n",
    "        retrieved_relevant += 1\n",
    "print('Precision: ' + str(retrieved_relevant / len(records_result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6900",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
